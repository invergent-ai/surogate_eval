# Surogate Eval

Surogate Eval is the core evaluation engine for the **Surogate LLMOps framework**.  
It provides a unified interface for benchmarking, security scanning (Red Teaming),
stress testing, and custom metric evaluation for Large Language Models.

---

## ðŸš€ Quick Start

### Installation

Install the package in editable mode from the root directory:

```bash
# Basic installation
pip install -e .

# Full installation including security and inference backends
pip install -e ".[security,inference]"
```

### Basic Usage

The framework is primarily driven by the `surogate-eval` CLI command.

```bash
# Run an evaluation using a config file
surogate-eval eval --config configs/my_eval.yaml

# List previous evaluation results
surogate-eval eval --list

# View a specific result file
surogate-eval eval --view results_20240114_120000.json

# Compare two evaluation runs
surogate-eval eval --compare run1.json run2.json
```

---

## ðŸ›  Features

**Multi-Target Evaluation**  
Evaluate multiple models (Local, API-based, or Custom) in a single run.

**Security & Guardrails**  
Integrated Red-Teaming via `deepteam` and automated guardrail validation.

**Benchmark Integration**  
Native support for standard benchmarks like **MMLU**, **GSM8K**, and more via `evalscope`.

**Stress Testing**  
Measure throughput, latency, and resource consumption under load.

**Distributed Execution**  
Automatic detection of multi-GPU setups using `torch.distributed`.

---

## ðŸ“‹ Configuration

Evaluations are defined in YAML configuration files.  
Below is a standard example:

```yaml
project:
  name: "Llama-3-Check"
  version: "1.0.0"

targets:
  - name: "llama3-8b"
    type: "local"
    model: "meta-llama/Meta-Llama-3-8B-Instruct"
    evaluations:
      - name: "General Knowledge"
        dataset: "data/general_qa.jsonl"
        metrics:
          - type: "g_eval"
          - type: "latency"
    red_teaming:
      enabled: true
      vulnerabilities:
        - injection
        - bias
```

---

## ðŸ“‚ Project Structure

The project follows the `src/` layout for robust packaging:

```text
.
â”œâ”€â”€ pyproject.toml           # Dependency and entry-point management
â”œâ”€â”€ src/
â”‚   â””â”€â”€ surogate_eval/       # Main package
â”‚       â”œâ”€â”€ eval.py          # The SurogateEval Orchestrator
â”‚       â”œâ”€â”€ cli/
â”‚       â”‚   â”œâ”€â”€ main.py      # Distributed CLI entry point
â”‚       â”‚   â””â”€â”€ eval.py      # Argument parsing and mode selection
â”‚       â”œâ”€â”€ backend/         # Execution backends (Local, Distributed)
â”‚       â”œâ”€â”€ benchmarks/      # Standard benchmark integrations
â”‚       â””â”€â”€ utils/           # Shared logging and command utilities
```

---

## ðŸ“Š Results & Reporting

All results are consolidated into a single JSON file stored by default in
`eval_results/`.

These files include:

- **Project Metadata**  
  Versioning and timestamps

- **Summary Statistics**  
  Aggregated scores across all targets

- **Detailed Metrics**  
  Per-test-case inputs, outputs, and scores

- **Security Logs**  
  Findings from red-teaming and guardrail tests

---

## ðŸ›¡ License

This project is licensed under the **AGPL-3.0 License**.
