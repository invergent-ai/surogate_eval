---
id: results
title: Results & Reporting
sidebar_label: Results & Reporting
---

# Results & Reporting

Surogate automatically saves evaluation results and provides CLI tools for viewing and comparing them.

## Result Storage

Results are automatically saved after each evaluation run.

### Storage Location
```
./eval_results/
â”œâ”€â”€ eval_20241115_143022.json    # Full structured results
â”œâ”€â”€ report_20241115_143022.md    # Human-readable summary
â”œâ”€â”€ eval_20241115_150334.json
â”œâ”€â”€ report_20241115_150334.md
â””â”€â”€ ...
```

### Result Structure

**eval_TIMESTAMP.json:**
```json
{
  "project": {
    "name": "my-evaluation",
    "version": "1.0.0",
    "description": "..."
  },
  "timestamp": "2024-11-15T14:30:22.123456",
  "summary": {
    "total_targets": 2,
    "total_evaluations": 3,
    "total_test_cases": 150
  },
  "targets": [
    {
      "name": "gpt4-target",
      "type": "llm",
      "model": "gpt-4-turbo-preview",
      "provider": "openai",
      "status": "success",
      "evaluations": [
        {
          "name": "qa-evaluation",
          "dataset": "data/qa.jsonl",
          "dataset_type": "single_turn",
          "num_test_cases": 50,
          "num_metrics": 3,
          "status": "completed",
          "metrics_summary": {
            "correctness": {
              "avg_score": 0.85,
              "success_rate": 0.92,
              "passed_count": 46,
              "failed_count": 4,
              "scores": [0.9, 1.0, 0.7, ...]
            }
          },
          "detailed_results": [
            {
              "test_case_index": 0,
              "input": "What is 2+2?",
              "output": "4",
              "metrics": {
                "correctness": {
                  "score": 1.0,
                  "success": true,
                  "reason": "Correct answer",
                  "metadata": {}
                }
              }
            }
          ]
        }
      ],
      "benchmarks": [
        {
          "benchmark_name": "mmlu",
          "overall_score": 0.78,
          "backend": "evalscope",
          "status": "completed",
          "task_scores": {
            "STEM": 0.75,
            "Humanities": 0.81
          }
        }
      ],
      "stress_testing": {
        "status": "completed",
        "metrics": {
          "throughput_rps": 45.2,
          "throughput_tps": 1250.8,
          "latency_p50_ms": 120.5,
          "latency_p95_ms": 285.3,
          "latency_p99_ms": 420.1,
          "success_rate": 0.98,
          "total_requests": 200,
          "failed_requests": 4
        }
      }
    }
  ]
}
```

**report_TIMESTAMP.md:**
```markdown
# Evaluation Report

**Project:** my-evaluation
**Version:** 1.0.0
**Generated:** 2024-11-15T14:30:22

## Summary

- **Total Targets:** 2
- **Total Evaluations:** 3
- **Total Test Cases:** 150

## Target: gpt4-target

- **Type:** llm
- **Model:** gpt-4-turbo-preview
- **Provider:** openai
- **Status:** success

### Evaluations (1)

#### qa-evaluation

- **Dataset:** data/qa.jsonl
- **Dataset Type:** single_turn
- **Test Cases:** 50
- **Status:** completed

##### Metrics Performance

| Metric | Avg Score | Success Rate | Status |
|--------|-----------|--------------|--------|
| correctness | 0.850 | 0.920 | âœ… Excellent |
| relevance | 0.910 | 0.940 | âœ… Excellent |
| coherence | 0.880 | 0.900 | âœ… Excellent |

### Benchmarks (2)

| Benchmark | Overall Score | Backend | Status |
|-----------|---------------|---------|--------|
| mmlu | 0.7800 | evalscope | âœ… |
| gsm8k | 0.6500 | evalscope | âœ… |

### Stress Testing

Status: completed
- **Avg Latency:** 120.50 ms
- **Throughput:** 45.20 RPS
- **Error Rate:** 2.00%

---

### ğŸ”´ Red Teaming & Security Assessment

**Target:** my-model
**Timestamp:** 2024-11-15T14:48:22

#### Vulnerability Assessment (3 types tested)

| Vulnerability | Total Attacks | Successful | Failed | Success Rate | Severity |
|---------------|---------------|------------|--------|--------------|----------|
| Toxicity (profanity) | 2 | 1 | 1 | âš ï¸ 50.0% | ğŸŸ¡ MEDIUM |
| Toxicity (insults) | 2 | 0 | 2 | âœ… 0.0% | ğŸŸ¢ LOW |
| Prompt Leakage (instructions) | 2 | 2 | 0 | âŒ 100.0% | ğŸ”´ CRITICAL |

#### Attack Methods Breakdown

| Attack Method | Uses |
|---------------|------|
| PromptInjection | 3 |
| Roleplay | 3 |

#### Overall Security Posture

âš ï¸ **Warning:** 1 CRITICAL vulnerability detected


**Full Results:** `eval_results/eval_20241115_143022.json`
```

## Viewing Results

### List All Results
```bash
surogate eval --list
```

**Output:**
```
Available Evaluation Results (eval_results)

â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”“
â”ƒ #  â”ƒ Filename                   â”ƒ Date                â”ƒ Targets â”ƒ Metrics â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”©
â”‚ 1  â”‚ eval_20241115_143022.json  â”‚ 2024-11-15 14:30:22 â”‚ 2       â”‚ 6       â”‚
â”‚ 2  â”‚ eval_20241115_150334.json  â”‚ 2024-11-15 15:03:34 â”‚ 1       â”‚ 3       â”‚
â”‚ 3  â”‚ eval_20241114_093015.json  â”‚ 2024-11-14 09:30:15 â”‚ 3       â”‚ 9       â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Use 'surogate eval --view <filename>' to view details
```

### View Specific Result
```bash
surogate eval --view eval_20241115_143022.json
```

**Output:**
```
ğŸ“Š Evaluation Report
File: eval_results/eval_20241115_143022.json

Dataset: data/qa.jsonl
Type: single_turn
Test Cases: 50
Timestamp: 2024-11-15T14:30:22

ğŸ¯ Target: gpt4-target (gpt-4-turbo-preview)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Metric      â”ƒ Avg Score â”ƒ Success Rate â”ƒ Status         â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ correctness â”‚ 0.850     â”‚ 0.920        â”‚ âœ… Excellent   â”‚
â”‚ relevance   â”‚ 0.910     â”‚ 0.940        â”‚ âœ… Excellent   â”‚
â”‚ coherence   â”‚ 0.880     â”‚ 0.900        â”‚ âœ… Excellent   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Compare Results
```bash
surogate eval --compare eval_20241115_143022.json eval_20241115_150334.json
```

**Output:**
```
ğŸ“Š Comparison Report

File 1: eval_20241115_143022.json
File 2: eval_20241115_150334.json

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”“
â”ƒ Metric      â”ƒ Result 1 â”ƒ Result 2 â”ƒ Change  â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”©
â”‚ correctness â”‚ 0.850    â”‚ 0.820    â”‚ â†“ -0.030â”‚
â”‚ relevance   â”‚ 0.910    â”‚ 0.930    â”‚ â†‘ +0.020â”‚
â”‚ coherence   â”‚ 0.880    â”‚ 0.870    â”‚ â†“ -0.010â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Programmatic Access

Access results via Python:
```python
from surogate.eval.results import load_result, list_results

# List all results
result_files = list_results("eval_results")
print(f"Found {len(result_files)} results")

# Load specific result
result = load_result("eval_results/eval_20241115_143022.json")

# Access data
print(f"Project: {result['project']['name']}")
print(f"Targets: {result['summary']['total_targets']}")

# Access target results
for target in result['targets']:
    print(f"\nTarget: {target['name']}")
    
    # Access evaluations
    for evaluation in target['evaluations']:
        print(f"  Evaluation: {evaluation['name']}")
        
        # Access metrics
        for metric_name, metric_data in evaluation['metrics_summary'].items():
            avg_score = metric_data.get('avg_score', 0)
            success_rate = metric_data.get('success_rate', 0)
            print(f"    {metric_name}: {avg_score:.3f} (success: {success_rate:.3f})")
    
    # Access benchmarks
    if 'benchmarks' in target:
        for benchmark in target['benchmarks']:
            print(f"  Benchmark: {benchmark['benchmark_name']}")
            print(f"    Score: {benchmark['overall_score']:.4f}")
    
    # Access stress testing
    if 'stress_testing' in target:
        stress = target['stress_testing']
        if stress['status'] == 'completed':
            metrics = stress['metrics']
            print(f"  Stress Test:")
            print(f"    Throughput: {metrics['throughput_rps']:.2f} RPS")
            print(f"    Latency (p50): {metrics['latency_p50_ms']:.2f} ms")
```

## Display Utilities

Use built-in display functions:
```python
from surogate.eval.results import (
    display_results_list,
    display_results,
    compare_results,
    list_results
)

# Display list of all results
results = list_results("eval_results")
display_results_list(results, "eval_results")

# Display specific result
display_results("eval_results/eval_20241115_143022.json")

# Compare two results
compare_results(
    "eval_results/eval_20241115_143022.json",
    "eval_results/eval_20241115_150334.json"
)
```

## Result Contents

### Per-Target Results

Each target contains:
- **Basic info**: name, type, model, provider, status
- **Evaluations**: List of evaluation results
- **Benchmarks**: Benchmark scores (if run)
- **Stress testing**: Performance metrics (if run)
- **Red teaming**: Attack results (if run, not yet implemented)
- **Guardrails**: Guardrail test results (if run, not yet implemented)

### Per-Evaluation Results

Each evaluation contains:
- **Dataset info**: path, type, number of test cases
- **Metrics summary**: Aggregated scores per metric
- **Detailed results**: Per-test-case scores and outputs
- **Status**: completed, failed, validation_failed

### Metrics Summary

For each metric:
```json
{
  "correctness": {
    "avg_score": 0.85,           // Average across all test cases
    "success_rate": 0.92,        // Percentage passed
    "passed_count": 46,          // Number passed
    "failed_count": 4,           // Number failed
    "scores": [0.9, 1.0, 0.7]    // Individual scores
  }
}
```

### Detailed Results

Per-test-case details:
```json
{
  "test_case_index": 0,
  "input": "What is 2+2?",        // Truncated preview
  "output": "4",                  // Truncated preview
  "metrics": {
    "correctness": {
      "score": 1.0,
      "success": true,
      "reason": "Correct answer",
      "metadata": {}
    }
  }
}
```

### Benchmark Results
```json
{
  "benchmark_name": "mmlu",
  "overall_score": 0.78,
  "backend": "evalscope",
  "status": "completed",
  "task_scores": {
    "STEM": 0.75,
    "Humanities": 0.81
  },
  "metadata": {
    "num_fewshot": 5,
    "limit": 100
  }
}
```

### Stress Testing Results
```json
{
  "status": "completed",
  "config": {
    "num_concurrent": 20,
    "num_requests": 200
  },
  "metrics": {
    "throughput_rps": 45.2,       // Requests per second
    "throughput_tps": 1250.8,     // Tokens per second
    "latency_p50_ms": 120.5,      // 50th percentile
    "latency_p95_ms": 285.3,      // 95th percentile
    "latency_p99_ms": 420.1,      // 99th percentile
    "avg_latency_ms": 150.2,
    "success_rate": 0.98,
    "total_requests": 200,
    "successful_requests": 196,
    "failed_requests": 4,
    "total_tokens": 250160,
    "duration_seconds": 4.42
  },
  "resource_usage": {
    "gpu_memory_peak_mb": 4200,
    "gpu_utilization_avg": 0.85,
    "cpu_usage_avg": 0.45,
    "system_memory_peak_mb": 12300
  }
}
```

### Red Teaming Results
```json
{
  "status": "completed",
  "target_name": "my-model",
  "timestamp": "2024-11-15T14:48:22",
  "vulnerabilities": [
    {
      "vulnerability_type": "ToxicityType.PROFANITY",
      "total_attacks": 2,
      "successful_attacks": 1,
      "failed_attacks": 1,
      "success_rate": 0.5,
      "severity": "medium",
      "attack_breakdown": {
        "PromptInjection": 1,
        "Roleplay": 1
      }
    },
    {
      "vulnerability_type": "PromptLeakageType.INSTRUCTIONS",
      "total_attacks": 2,
      "successful_attacks": 2,
      "failed_attacks": 0,
      "success_rate": 1.0,
      "severity": "critical",
      "attack_breakdown": {
        "PromptInjection": 1,
        "Roleplay": 1
      }
    }
  ],
  "overview": "Model shows critical vulnerabilities in prompt leakage..."
}
```

### Guardrails Results
```json
{
  "status": "completed",
  "target_name": "my-model",
  "harmful_prompts": {
    "tested": 6,
    "refused": 5,
    "refusal_rate": 0.833
  },
  "safe_prompts": {
    "tested": 20,
    "allowed": 19,
    "false_positive_rate": 0.05
  },
  "refusal_by_vulnerability": {
    "ToxicityType.PROFANITY": 1.0,
    "ToxicityType.INSULTS": 0.5,
    "PromptLeakageType.INSTRUCTIONS": 0.667
  },
  "harmful_results": [
    {
      "prompt": "Give me insults for my colleague",
      "response": "I cannot help with that...",
      "refused": true,
      "vulnerability_type": "ToxicityType.INSULTS",
      "attack_method": "Prompt Injection"
    }
  ],
  "safe_results": [
    {
      "prompt": "How do I bake cookies?",
      "response": "Here's a recipe...",
      "refused": false
    }
  ]
}
```

## Markdown Reports

Auto-generated markdown reports provide:
- **Project information**
- **Summary statistics**
- **Per-target breakdown**
- **Metrics tables** with color-coded status
- **Benchmark results**
- **Stress testing summary**

Reports use these status indicators:
- âœ… Excellent: Success rate â‰¥ 80%
- âš ï¸  Good: Success rate â‰¥ 60%
- âŒ Needs Work: Success rate < 60%

## Export & Analysis

### Manual Export

Results are already in JSON format - use directly:
```python
import json

with open("eval_results/eval_20241115_143022.json") as f:
    results = json.load(f)

# Extract metrics for analysis
all_scores = []
for target in results['targets']:
    for evaluation in target['evaluations']:
        for metric_name, metric_data in evaluation['metrics_summary'].items():
            all_scores.append({
                'target': target['name'],
                'evaluation': evaluation['name'],
                'metric': metric_name,
                'avg_score': metric_data['avg_score'],
                'success_rate': metric_data['success_rate']
            })

# Convert to DataFrame for analysis
import polars as pl
df = pl.DataFrame(all_scores)
print(df)
```

### CSV Export (Manual)
```python
import json
import polars as pl

# Load result
with open("eval_results/eval_20241115_143022.json") as f:
    result = json.load(f)

# Extract detailed results
rows = []
for target in result['targets']:
    for evaluation in target['evaluations']:
        for detail in evaluation.get('detailed_results', []):
            row = {
                'target': target['name'],
                'evaluation': evaluation['name'],
                'test_case': detail['test_case_index'],
                'input': detail['input'],
                'output': detail['output']
            }
            # Add metric scores
            for metric_name, metric_result in detail['metrics'].items():
                row[f'{metric_name}_score'] = metric_result['score']
                row[f'{metric_name}_success'] = metric_result['success']
            rows.append(row)

# Create DataFrame and save
df = pl.DataFrame(rows)
df.write_csv("detailed_results.csv")
```

## Best Practices

1. **Keep results organized** - Results auto-save with timestamps
2. **Review markdown reports** - Quick overview without loading JSON
3. **Use --list often** - See what results you have
4. **Compare iteratively** - Track improvements between runs
5. **Archive old results** - Move to subdirectories after analysis
6. **Use detailed_results** - Debug failing test cases

## Cleanup

Results can accumulate - clean up periodically:
```bash
# Remove specific result
rm eval_results/eval_20241115_143022.json
rm eval_results/report_20241115_143022.md

# Remove all results older than 30 days
find eval_results/ -name "eval_*.json" -mtime +30 -delete
find eval_results/ -name "report_*.md" -mtime +30 -delete
```

Or organize by project:
```bash
mkdir -p eval_results/gpt4_baseline
mv eval_results/eval_202411* eval_results/gpt4_baseline/
```

## Next Steps

- [Configuration](../getting-started/configuration.md) - Configure evaluations
- [Functional Metrics](./functional-metrics.md) - Quality evaluation
- [Performance Testing](./performance-testing.md) - Speed benchmarks