# surogate/eval/eval.py
import asyncio
import os
from pathlib import Path
from typing import Dict, Any, List

from surogate_eval.backend import LocalBackend
from surogate_eval.benchmarks import BenchmarkRegistry
from surogate_eval.config.eval_config import TargetConfig, EvalConfig
from surogate_eval.datasets import DatasetLoader, DatasetValidator
from surogate_eval.targets import BaseTarget, TargetFactory
from surogate_eval.utils.command import SurogateCommand
from surogate_eval.utils.logger import get_logger

logger = get_logger()

os.environ["DEEPEVAL_TELEMETRY_OPT_OUT"] = "1"
os.environ["DEEPEVAL_FILE_SYSTEM"] = "READ_ONLY"
os.environ['EVALSCOPE_CACHE'] = os.path.join(os.path.expanduser('~'), '.cache', 'evalscope')
os.environ['MODELSCOPE_TRUST_REMOTE_CODE'] = '1'

class SurogateEval(SurogateCommand):
    def __init__(self, *, config, args):
        super().__init__(config=config, args=args)

        self.consolidated_results = {
            "project": {},
            "timestamp": None,
            "summary": {
                "total_targets": 0,
                "total_evaluations": 0,
                "total_test_cases": 0
            },
            "targets": []
        }
        self.targets: List[BaseTarget] = []

    def run(self):
        """Run the evaluation pipeline."""
        from datetime import datetime

        logger.banner("SUROGATE EVAL")

        self.consolidated_results["timestamp"] = datetime.now().isoformat()

        # Store project info directly from config
        self.consolidated_results["project"] = {
            "name": self.config.project.name,
            "version": self.config.project.version,
            "description": self.config.project.description
        }

        # Process targets
        try:
            self._process_targets()
        finally:
            self._cleanup()

        # Save results
        self._save_consolidated_results()
        logger.success("Surogate Eval completed")

    def _process_targets(self):
        """Process all targets from config."""
        target_configs = self.config.targets  # Direct attribute access

        if not target_configs:
            logger.warning("No targets specified in configuration")
            return

        logger.info(f"Processing {len(target_configs)} target(s)")

        self.consolidated_results["summary"]["total_targets"] = len(target_configs)

        # PHASE 1: Create ALL targets first (so judge targets exist for evaluations)
        logger.info("Creating all targets...")
        for target_config in target_configs:
            target_name = target_config.name or 'unnamed'
            try:
                logger.info(f"Creating target: {target_name}")

                # Convert TargetConfig to dict for TargetFactory
                target_dict = target_config.to_dict()
                target = TargetFactory.create_target(target_dict)

                # Health check
                if not target.health_check():
                    logger.error(f"Target '{target_name}' health check failed")
                    self.consolidated_results["targets"].append({
                        "name": target_name,
                        "status": "unhealthy",
                        "evaluations": []
                    })
                    continue

                logger.success(f"Target '{target_name}' is healthy")
                self.targets.append(target)

            except Exception as e:
                logger.error(f"Failed to create target '{target_name}': {e}")
                self.consolidated_results["targets"].append({
                    "name": target_name,
                    "status": "failed",
                    "error": str(e),
                    "evaluations": []
                })

        # PHASE 2: Now run evaluations on each target
        logger.info("Running evaluations on all targets...")
        for target_config in target_configs:
            target_name = target_config.name or 'unnamed'

            # Find the created target
            target = self._find_target_by_name(target_name)
            if not target:
                logger.warning(f"Skipping evaluations for '{target_name}' (target not healthy)")
                continue

            logger.separator(char="‚ïê")
            logger.header(f"Target: {target_name}")
            logger.separator(char="‚ïê")

            try:
                target_results = self._run_target_evaluations(target, target_config)
                if target_results:
                    # Check if this target already has a result entry (from phase 1)
                    existing_idx = None
                    for idx, t in enumerate(self.consolidated_results["targets"]):
                        if t.get("name") == target_name:
                            existing_idx = idx
                            break

                    if existing_idx is not None:
                        # Update existing entry
                        self.consolidated_results["targets"][existing_idx] = target_results
                    else:
                        # Add new entry
                        self.consolidated_results["targets"].append(target_results)

            except Exception as e:
                logger.error(f"Failed to run evaluations for target '{target_name}': {e}")
                import traceback
                traceback.print_exc()

    def _run_target_evaluations(self, target: BaseTarget, target_config: TargetConfig) -> Dict[str, Any]:
        """
        Run all evaluations for a single target.

        Args:
            target: Already-created target instance
            target_config: Target configuration (TargetConfig dataclass)

        Returns:
            Target results dictionary
        """
        target_name = target.name

        # Initialize target result structure
        target_result = {
            "name": target_name,
            "type": target.target_type.value,
            "model": target.config.get('model', 'unknown'),
            "provider": target.config.get('provider', 'unknown'),
            "status": "success",
            "evaluations": []
        }

        # Setup backend (if infrastructure specified)
        backend = self._setup_target_backend(target_config)

        # Run evaluations
        evaluations = target_config.evaluations or []
        if evaluations:
            logger.info(f"Running {len(evaluations)} evaluation(s) for target '{target_name}'")
            self.consolidated_results["summary"]["total_evaluations"] += len(evaluations)

            for eval_config in evaluations:
                eval_result = self._run_evaluation(target, eval_config, backend)
                if eval_result:
                    target_result["evaluations"].append(eval_result)
                    self.consolidated_results["summary"]["total_test_cases"] += eval_result.get("num_test_cases", 0)
        else:
            logger.warning(f"No evaluations specified for target '{target_name}'")

        # Run benchmarks
        for eval_config in evaluations:
            benchmarks_config = eval_config.get('benchmarks', [])
            if benchmarks_config:
                logger.info(f"Running {len(benchmarks_config)} benchmark(s)")
                benchmark_results = self._run_benchmarks(target, benchmarks_config)
                if benchmark_results:
                    target_result['benchmarks'] = benchmark_results

        # Run stress testing
        stress_testing = target_config.stress_testing or {}
        if stress_testing.get('enabled'):
            logger.info(f"Running stress testing for target '{target_name}'")
            stress_result = self._run_stress_testing(target, stress_testing)
            if stress_result:
                target_result["stress_testing"] = stress_result

        # Run async security tests together in one event loop
        async def run_security_tests():
            """Run all security tests in a single async context."""
            results = {}

            # Run red teaming - ONLY if enabled
            red_teaming = target_config.red_teaming or {}
            if red_teaming.get('enabled'):
                logger.info(f"Running red teaming for target '{target_name}'")
                red_team_result = await self._run_red_teaming_async(target, red_teaming)
                if red_team_result:
                    results['red_teaming'] = red_team_result

            # Run guardrails - ONLY if enabled
            guardrails = target_config.guardrails or {}
            if guardrails.get('enabled'):
                logger.info(f"Testing guardrails for target '{target_name}'")
                guardrails_result = await self._run_guardrails_testing_async(target, guardrails)
                if guardrails_result:
                    results['guardrails'] = guardrails_result

            return results

        # Run all async security tests in one event loop (if any are enabled)
        red_teaming = target_config.red_teaming or {}
        guardrails = target_config.guardrails or {}

        if red_teaming.get('enabled') or guardrails.get('enabled'):
            security_results = asyncio.run(run_security_tests())

            if 'red_teaming' in security_results:
                target_result["red_teaming"] = security_results['red_teaming']
            if 'guardrails' in security_results:
                target_result["guardrails"] = security_results['guardrails']

        # Shutdown backend
        if backend:
            backend.shutdown()

        return target_result

    def _setup_target_backend(self, target_config: TargetConfig) -> Any:
        """
        Setup execution backend for a target.

        Args:
            target_config: Target configuration (TargetConfig dataclass)

        Returns:
            Backend instance or None
        """
        infra_config = target_config.infrastructure or {}

        if not infra_config:
            logger.debug("No infrastructure config - using default")
            return None

        backend_type = infra_config.get('backend', 'local')

        if backend_type == 'local':
            backend = LocalBackend(infra_config)
            logger.success(f"Local backend initialized with {infra_config.get('workers', 1)} workers")
            return backend
        else:
            raise NotImplementedError(f"Backend '{backend_type}' not implemented yet")

    def _run_evaluation(
            self,
            target: BaseTarget,
            eval_config: Dict[str, Any],
            backend: Any = None
    ) -> Dict[str, Any]:
        """
        Run a single evaluation on a target.
        """

        eval_name = eval_config.get('name', 'unnamed')
        dataset_path = eval_config.get('dataset')

        logger.separator(char="‚îÄ")
        logger.header(f"Evaluation: {eval_name}")
        logger.info(f"Dataset: {dataset_path}")
        logger.separator(char="‚îÄ")

        if not dataset_path:
            logger.warning(f"No dataset specified for evaluation '{eval_name}'")
            return None

        try:
            # Load metrics config first
            metric_configs = eval_config.get('metrics', [])
            if not metric_configs:
                logger.warning(f"No metrics specified for evaluation '{eval_name}'")
                return None

            # Calculate max limit for dataset loading
            max_limit = None
            all_have_limits = True
            for mc in metric_configs:
                metric_limit = mc.get('limit')
                if metric_limit is None:
                    all_have_limits = False
                    break
                elif max_limit is None or metric_limit > max_limit:
                    max_limit = metric_limit

            dataset_limit = max_limit if all_have_limits else None

            # Load dataset once
            loader = DatasetLoader()
            dataset_type = loader.detect_dataset_type(dataset_path)
            logger.info(f"Dataset type: {dataset_type}")

            all_test_cases = loader.load_test_cases(dataset_path, limit=dataset_limit)
            logger.info(f"Loaded {len(all_test_cases)} test cases")

            # Validate dataset
            validator = DatasetValidator()
            df = loader.load(dataset_path, limit=dataset_limit)
            is_valid, errors = validator.validate(df)

            if not is_valid:
                logger.error("Dataset validation failed:")
                for error in errors:
                    logger.error(f"  - {error}")
                return {
                    "name": eval_name,
                    "dataset": dataset_path,
                    "dataset_type": dataset_type,
                    "status": "validation_failed",
                    "errors": errors
                }

            # Filter metrics by dataset type
            filtered_metric_configs = self._filter_metrics_by_dataset_type(
                metric_configs,
                dataset_type
            )

            if not filtered_metric_configs:
                logger.error(f"No compatible metrics for dataset type: {dataset_type}")
                return None

            logger.info(f"Using {len(filtered_metric_configs)} metric(s)")

            from surogate_eval.metrics import MetricRegistry
            metrics = MetricRegistry.create_metrics(filtered_metric_configs)

            # Cache for inference results: index -> (output, response)
            inference_cache = {}

            def get_inference(idx: int):
                """Get or compute inference for test case at index."""
                if idx in inference_cache:
                    return inference_cache[idx]

                test_case = all_test_cases[idx]
                try:
                    from surogate_eval.targets.base import TargetRequest
                    from surogate_eval.datasets.test_case import TestCase, MultiTurnTestCase

                    if isinstance(test_case, TestCase):
                        request = TargetRequest(prompt=test_case.input)
                    elif isinstance(test_case, MultiTurnTestCase):
                        request = TargetRequest(messages=test_case.get_context())
                    else:
                        logger.error(f"Unknown test case type: {type(test_case)}")
                        inference_cache[idx] = ("", None)
                        return inference_cache[idx]

                    response = target.send_request(request)
                    inference_cache[idx] = (response.content, response)

                except Exception as e:
                    logger.error(f"Failed to get output for test case {idx}: {e}")
                    inference_cache[idx] = ("", None)

                return inference_cache[idx]

            # Run metrics - each with its own limit
            metric_results = {}
            detailed_results = {}  # Use dict keyed by index for sparse results

            for metric, metric_config in zip(metrics, filtered_metric_configs):
                logger.info(f"Running metric: {metric.name}")

                # Get this metric's limit
                metric_limit = metric_config.get('limit')

                # Determine how many test cases for this metric
                if metric_limit is not None:
                    num_cases = min(metric_limit, len(all_test_cases))
                else:
                    num_cases = len(all_test_cases)

                logger.info(f"  Running inference on {num_cases} test cases")

                # Get test cases and run inference for this metric
                metric_test_cases = all_test_cases[:num_cases]
                metric_outputs = []
                metric_responses = []

                for idx in range(num_cases):
                    output, response = get_inference(idx)
                    metric_outputs.append(output)
                    metric_responses.append(response)

                    if (idx + 1) % 10 == 0:
                        logger.step(idx + 1, num_cases, f"Progress: {idx + 1}/{num_cases}")

                try:
                    # Set judge target if needed
                    from surogate_eval.metrics import LLMJudgeMetric
                    if isinstance(metric, LLMJudgeMetric):
                        judge_config = metric.config.get('judge_model', {})
                        judge_target_name = judge_config.get('target')

                        if judge_target_name:
                            judge_target = self._find_target_by_name(judge_target_name)
                            if judge_target:
                                metric.set_judge_target(judge_target)
                                logger.debug(f"Set judge target '{judge_target_name}'")
                            else:
                                logger.warning(f"Judge target '{judge_target_name}' not found")

                    # Evaluate batch
                    batch_result = metric.evaluate_batch(
                        metric_test_cases,
                        metric_outputs,
                        metric_responses
                    )

                    # Store aggregated results
                    metric_results[metric.name] = batch_result.to_dict()

                    # Store detailed per-test-case results
                    for i, individual_result in enumerate(batch_result.results):
                        if i not in detailed_results:
                            from surogate_eval.datasets.test_case import TestCase
                            if isinstance(metric_test_cases[i], TestCase):
                                input_text = metric_test_cases[i].input
                            else:
                                input_text = [{"role": turn.role, "content": turn.content} for turn in
                                              metric_test_cases[i].turns]

                            detailed_results[i] = {
                                'test_case_index': i,
                                'input': input_text,
                                'output': metric_outputs[i] or "",
                                'metrics': {}
                            }

                        detailed_results[i]['metrics'][metric.name] = {
                            'score': individual_result.score,
                            'success': individual_result.success,
                            'reason': individual_result.reason,
                            'metadata': individual_result.metadata
                        }

                    logger.metric(f"{metric.name} - Avg Score", f"{batch_result.avg_score:.3f}")
                    logger.metric(f"{metric.name} - Success Rate", f"{batch_result.success_rate:.3f}")

                except Exception as e:
                    logger.error(f"Metric {metric.name} failed: {e}")
                    import traceback
                    logger.debug(traceback.format_exc())
                    metric_results[metric.name] = {
                        'error': str(e),
                        'status': 'failed'
                    }

            # Convert detailed_results dict to sorted list
            detailed_results_list = [detailed_results[i] for i in sorted(detailed_results.keys())]

            # Create evaluation result
            return {
                "name": eval_name,
                "dataset": dataset_path,
                "dataset_type": dataset_type,
                "num_test_cases": len(inference_cache),  # Actual number of inferences run
                "num_metrics": len(metrics),
                "status": "completed",
                "metrics_summary": metric_results,
                "detailed_results": detailed_results_list
            }

        except Exception as e:
            logger.error(f"Failed to run evaluation '{eval_name}': {e}")
            import traceback
            traceback.print_exc()

            return {
                "name": eval_name,
                "dataset": dataset_path,
                "status": "failed",
                "error": str(e)
            }

    def _run_benchmarks(
            self,
            target: BaseTarget,
            benchmark_configs: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Run benchmarks on target.

        Args:
            target: Target to evaluate
            benchmark_configs: List of benchmark configurations

        Returns:
            List of benchmark results
        """
        if not benchmark_configs:
            return []

        logger.separator(char="‚îÄ")
        logger.header(f"Running {len(benchmark_configs)} Benchmark(s)")
        logger.separator(char="‚îÄ")

        benchmark_results = []

        for bench_config in benchmark_configs:
            bench_result = self._run_single_benchmark(target, bench_config)
            if bench_result:
                benchmark_results.append(bench_result)

        return benchmark_results

    def _run_single_benchmark(
            self,
            target: BaseTarget,
            bench_config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Run a single benchmark on target."""
        benchmark_name = bench_config.get('name')
        logger.info(f"Running benchmark: {benchmark_name}")

        try:
            from surogate_eval.benchmarks import BenchmarkConfig

            config = BenchmarkConfig(
                name=benchmark_name,
                backend=bench_config.get('backend', 'evalscope'),
                # New custom dataset fields
                source=bench_config.get('source'),
                task_type=bench_config.get('task_type'),
                columns=bench_config.get('columns', {}),
                choices_columns=bench_config.get('choices_columns'),
                choices_labels=bench_config.get('choices_labels'),
                split=bench_config.get('split', 'test'),
                prompt_template=bench_config.get('prompt_template'),
                stop_sequences=bench_config.get('stop_sequences'),
                # Existing fields
                path=bench_config.get('path'),
                num_fewshot=bench_config.get('num_fewshot'),
                limit=bench_config.get('limit'),
                tasks=bench_config.get('tasks'),
                subset=bench_config.get('subset'),
                use_cache=bench_config.get('use_cache', True),
                cache_dir=bench_config.get('cache_dir'),
                backend_params=bench_config.get('backend_params', {}),
                dataset_hub=bench_config.get('dataset_hub'),
                tokenizer=bench_config.get('tokenizer'),
                batch_size=bench_config.get('batch_size'),
                max_tokens=bench_config.get('max_tokens'),
                temperature=bench_config.get('temperature'),
                system_prompt=bench_config.get('system_prompt'),
                num_concurrent=bench_config.get('num_concurrent'),
                log_samples=bench_config.get('log_samples', True),
                judge_model=bench_config.get('judge_model'),
                judge_criteria=bench_config.get('judge_criteria'),
            )

            # Create benchmark instance
            benchmark = BenchmarkRegistry.create_benchmark(config)

            # Get judge target if specified
            judge_model_config = bench_config.get('judge_model')
            if judge_model_config:
                judge_target_name = judge_model_config.get('target')
                judge_target = self._find_target_by_name(judge_target_name)
                if judge_target:
                    # Pass judge to backend via backend_params
                    benchmark.config.backend_params['judge_target'] = judge_target
                    logger.info(f"Using judge '{judge_target_name}' for benchmark '{benchmark_name}'")
                else:
                    logger.warning(f"Judge target '{judge_target_name}' not found")

            # Validate target compatibility
            if not benchmark.validate_target(target):
                target_type = target.target_type.value
                required = benchmark.REQUIRED_TARGET_TYPES
                logger.error(
                    f"Target '{target.name}' (type: {target_type}) not compatible with "
                    f"benchmark '{benchmark_name}' (requires: {required})"
                )
                return {
                    'benchmark': benchmark_name,
                    'status': 'incompatible',
                    'error': f'Benchmark requires {required} target, got {target_type}'
                }

            # Run benchmark
            result = benchmark.evaluate(target)

            # Convert to dict
            result_dict = result.to_dict()
            result_dict['status'] = 'completed'

            logger.success(f"Benchmark '{benchmark_name}' completed")
            logger.metric(f"{benchmark_name} - Overall Score", f"{result.overall_score:.4f}")

            return result_dict

        except Exception as e:
            logger.error(f"Benchmark '{benchmark_name}' failed: {e}")
            import traceback
            logger.debug(traceback.format_exc())

            return {
                'benchmark': benchmark_name,
                'status': 'failed',
                'error': str(e)
            }

    def _run_stress_testing(self, target: BaseTarget, stress_config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Run stress testing on target.

        Args:
            target: Target to stress test
            stress_config: Stress testing configuration (dict)

        Returns:
            Stress test results
        """
        from surogate_eval.metrics.stress import StressTester, StressTestConfig
        from surogate_eval.datasets import DatasetLoader

        logger.info(f"Running stress test for target '{target.name}'")

        try:
            # Load test dataset
            dataset_path = stress_config.get('dataset')
            if not dataset_path:
                logger.error("No dataset specified for stress testing")
                return {"status": "error", "reason": "No dataset specified"}

            loader = DatasetLoader()
            test_cases = loader.load_test_cases(dataset_path)

            logger.info(f"Loaded {len(test_cases)} test cases for stress testing")

            # Create stress test config
            config = StressTestConfig(
                num_concurrent=stress_config.get('num_concurrent', 10),
                duration_seconds=stress_config.get('duration_seconds'),
                num_requests=stress_config.get('num_requests', 100),
                progressive=stress_config.get('progressive', False),
                start_concurrent=stress_config.get('start_concurrent', 1),
                step_concurrent=stress_config.get('step_concurrent', 2),
                step_duration_seconds=stress_config.get('step_duration_seconds', 30),
                monitor_resources=stress_config.get('monitor_resources', True),
                warmup_requests=stress_config.get('warmup_requests', 5),
            )

            # Run stress test
            tester = StressTester(target, test_cases)
            result = tester.run(config)

            return result.to_dict()

        except Exception as e:
            logger.error(f"Stress testing failed: {e}")
            import traceback
            traceback.print_exc()
            return {"status": "error", "reason": str(e)}

    def _find_target_by_name(self, name: str) -> BaseTarget:
        """Find a target by name from created targets."""
        for target in self.targets:
            if target.name == name:
                return target
        return None

    def _filter_metrics_by_dataset_type(
            self,
            metric_configs: List[Dict[str, Any]],
            dataset_type: str
    ) -> List[Dict[str, Any]]:
        """
        Filter metrics based on dataset type compatibility.

        Args:
            metric_configs: List of metric configurations
            dataset_type: 'single_turn' or 'multi_turn'

        Returns:
            Filtered list of compatible metric configurations
        """
        single_turn_metrics = {
            'g_eval',
            'dag',
            'multimodal_g_eval',
            'toxicity',
            'bias',
            'harm',
            'embedding_similarity',
            'classification',
            'latency',
            'throughput',
            'token_generation_speed',
        }

        multi_turn_metrics = {
            'conversational_g_eval',
            'conversation_coherence',
            'context_retention',
            'turn_analysis',
            'conversational_dag',
            'multimodal_g_eval',
            'toxicity',
            'bias',
            'harm',
            'latency',
            'throughput',
            'token_generation_speed',
        }

        filtered_configs = []
        skipped_metrics = []

        for config in metric_configs:
            metric_type = config.get('type')
            metric_name = config.get('name', metric_type)

            is_compatible = False

            if dataset_type == 'single_turn':
                is_compatible = metric_type in single_turn_metrics
            elif dataset_type == 'multi_turn':
                is_compatible = metric_type in multi_turn_metrics

            if is_compatible:
                filtered_configs.append(config)
                logger.debug(f"Metric '{metric_name}' is compatible with {dataset_type}")
            else:
                skipped_metrics.append(metric_name)
                logger.warning(f"Skipping metric '{metric_name}' - incompatible with {dataset_type}")

        if skipped_metrics:
            logger.info(f"Skipped {len(skipped_metrics)} incompatible metrics: {', '.join(skipped_metrics)}")

        return filtered_configs

    async def _run_red_teaming_async(self, target: BaseTarget, red_team_config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Run red teaming tests on target (async version).

        Args:
            target: Target to test
            red_team_config: Red teaming configuration (dict)

        Returns:
            Red teaming results
        """
        from surogate_eval.security import RedTeamRunner, RedTeamConfig
        from surogate_eval.models import DeepEvalTargetWrapper

        logger.info(f"Running red-team scan for target '{target.name}'")

        try:
            # Resolve simulator_model - can be string OR target reference
            simulator_model = red_team_config.get('simulator_model', 'gpt-4o-mini')
            if isinstance(simulator_model, dict) and simulator_model.get('target'):
                sim_target = self._find_target_by_name(simulator_model['target'])
                if sim_target:
                    simulator_model = DeepEvalTargetWrapper(sim_target)
                    logger.info(f"Using target '{simulator_model.get_model_name()}' as simulator model")
                else:
                    logger.warning(
                        f"Simulator target '{simulator_model['target']}' not found, using default 'gpt-4o-mini'")
                    simulator_model = 'gpt-4o-mini'

            # Resolve evaluation_model - can be string OR target reference
            evaluation_model = red_team_config.get('evaluation_model', 'gpt-4o-mini')
            if isinstance(evaluation_model, dict) and evaluation_model.get('target'):
                eval_target = self._find_target_by_name(evaluation_model['target'])
                if eval_target:
                    evaluation_model = DeepEvalTargetWrapper(eval_target)
                    logger.info(f"Using target '{evaluation_model.get_model_name()}' as evaluation model")
                else:
                    logger.warning(
                        f"Evaluation target '{evaluation_model['target']}' not found, using default 'gpt-4o-mini'")
                    evaluation_model = 'gpt-4o-mini'

            # Create config
            config = RedTeamConfig(
                vulnerabilities=red_team_config.get('vulnerabilities', []),
                vulnerability_types=red_team_config.get('vulnerability_types', {}),
                attacks=red_team_config.get('attacks', []),
                attacks_per_vulnerability=red_team_config.get('attacks_per_vulnerability', 3),
                max_concurrent=red_team_config.get('max_concurrent', 10),
                run_async=red_team_config.get('run_async', True),
                simulator_model=simulator_model,
                evaluation_model=evaluation_model,
                purpose=red_team_config.get('purpose'),
                ignore_errors=red_team_config.get('ignore_errors', False)
            )

            # Run red-teaming
            runner = RedTeamRunner(target, config)

            translator_config = red_team_config.get('translator')
            if translator_config:
                translator_target_name = translator_config.get('target')
                if translator_target_name:
                    translator_target = self._find_target_by_name(translator_target_name)
                    if translator_target:
                        runner.set_translator(translator_target)
                        logger.info(f"Using translator target '{translator_target_name}'")
                    else:
                        logger.warning(f"Translator target '{translator_target_name}' not found")

            risk_assessment = await runner.run()

            return risk_assessment.to_dict()

        except Exception as e:
            logger.error(f"Red-teaming failed: {e}")
            import traceback
            logger.debug(traceback.format_exc())

            return {
                "status": "failed",
                "error": str(e)
            }

    async def _run_guardrails_testing_async(self, target: BaseTarget, guardrails_config: Dict[str, Any]) -> Dict[
        str, Any]:
        """
        Test guardrails on target (async version).

        Args:
            target: Target to test
            guardrails_config: Guardrails configuration (dict)

        Returns:
            Guardrails test results
        """
        from surogate_eval.security import GuardrailsEvaluator, GuardrailsConfig
        from surogate_eval.models import DeepEvalTargetWrapper

        logger.info(f"Testing guardrails for target '{target.name}'")

        try:
            # Resolve simulator_model - can be string OR target reference
            simulator_model = guardrails_config.get('simulator_model', 'gpt-3.5-turbo')
            if isinstance(simulator_model, dict) and simulator_model.get('target'):
                sim_target = self._find_target_by_name(simulator_model['target'])
                if sim_target:
                    simulator_model = DeepEvalTargetWrapper(sim_target)
                    logger.info(f"Using target '{simulator_model.get_model_name()}' as simulator model")
                else:
                    logger.warning(
                        f"Simulator target '{simulator_model['target']}' not found, using default 'gpt-3.5-turbo'")
                    simulator_model = 'gpt-3.5-turbo'

            # Resolve evaluation_model - can be string OR target reference
            evaluation_model = guardrails_config.get('evaluation_model', 'gpt-4o-mini')
            if isinstance(evaluation_model, dict) and evaluation_model.get('target'):
                eval_target = self._find_target_by_name(evaluation_model['target'])
                if eval_target:
                    evaluation_model = DeepEvalTargetWrapper(eval_target)
                    logger.info(f"Using target '{evaluation_model.get_model_name()}' as evaluation model")
                else:
                    logger.warning(
                        f"Evaluation target '{evaluation_model['target']}' not found, using default 'gpt-4o-mini'")
                    evaluation_model = 'gpt-4o-mini'

            # Create config
            config = GuardrailsConfig(
                vulnerabilities=guardrails_config.get('vulnerabilities', []),
                vulnerability_types=guardrails_config.get('vulnerability_types', {}),
                attacks=guardrails_config.get('attacks', []),
                attacks_per_vulnerability=guardrails_config.get('attacks_per_vulnerability', 3),
                safe_prompts_dataset=guardrails_config.get('safe_prompts_dataset'),
                refusal_judge_model_target=guardrails_config.get('refusal_judge_model', {}).get('target'),
                max_concurrent=guardrails_config.get('max_concurrent', 10),
                simulator_model=simulator_model,
                evaluation_model=evaluation_model,
                purpose=guardrails_config.get('purpose'),
                ignore_errors=guardrails_config.get('ignore_errors', False)
            )

            # Get judge target if specified
            judge_target = None
            if config.refusal_judge_model_target:
                judge_target = self._find_target_by_name(config.refusal_judge_model_target)
                if not judge_target:
                    logger.warning(f"Judge target '{config.refusal_judge_model_target}' not found")
                else:
                    logger.info(f"Using target '{judge_target.name}' as refusal judge")

            # Run guardrails evaluation
            evaluator = GuardrailsEvaluator(target, config, judge_target)
            result = await evaluator.evaluate()

            return result.to_dict()

        except Exception as e:
            logger.error(f"Guardrails testing failed: {e}")
            import traceback
            logger.debug(traceback.format_exc())

            return {
                "status": "failed",
                "error": str(e)
            }

    def _save_consolidated_results(self):
        """Save consolidated results to a single file."""
        try:
            from pathlib import Path
            import json
            from datetime import datetime
            from enum import Enum

            # Custom JSON encoder for Enums and other non-serializable types
            def custom_encoder(obj):
                if isinstance(obj, Enum):
                    return obj.value  # Convert Enum to its value
                # Handle dict with Enum keys
                if isinstance(obj, dict):
                    return {(k.value if isinstance(k, Enum) else k): v for k, v in obj.items()}
                return str(obj)  # Fallback for other types

            # Convert any Enum keys in the results to strings
            def convert_enum_keys(obj):
                """Recursively convert Enum keys to their values in dicts."""
                if isinstance(obj, dict):
                    return {
                        (k.value if isinstance(k, Enum) else k): convert_enum_keys(v)
                        for k, v in obj.items()
                    }
                elif isinstance(obj, list):
                    return [convert_enum_keys(item) for item in obj]
                elif isinstance(obj, Enum):
                    return obj.value
                else:
                    return obj

            # Convert the entire results structure
            serializable_results = convert_enum_keys(self.consolidated_results)

            # Create results directory
            results_dir = Path("eval_results")
            results_dir.mkdir(exist_ok=True)

            # Generate filename with job ID if available, otherwise use timestamp
            job_id = os.environ.get('EVAL_JOB_ID') or os.environ.get('TASK_RUN_ID')
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

            if job_id:
                filename = f"eval_{job_id}.json"
                report_filename = f"report_{job_id}.md"
            else:
                filename = f"eval_{timestamp}.json"
                report_filename = f"report_{timestamp}.md"

            filepath = results_dir / filename

            # Save JSON results with custom encoder
            with open(filepath, 'w') as f:
                json.dump(serializable_results, f, indent=2, default=custom_encoder)

            logger.separator(char="‚ïê")
            logger.success(f"Consolidated results saved to: {filepath}")
            logger.separator(char="‚ïê")

            # Create summary report
            self._create_summary_report(serializable_results, results_dir, job_id or timestamp)

        except Exception as e:
            logger.error(f"Failed to save consolidated results: {e}")
            import traceback
            logger.debug(traceback.format_exc())

    def _create_summary_report(
            self,
            results: Dict[str, Any],
            results_dir: Path,
            timestamp: str
    ):
        """
        Create a human-readable summary report.

        Args:
            results: Consolidated results dictionary
            results_dir: Directory to save report
            timestamp: Timestamp for filename
        """
        try:
            report_file = results_dir / f"report_{timestamp}.md"

            with open(report_file, 'w') as f:
                # Header
                f.write(f"# Evaluation Report\n\n")
                project_info = results.get('project', {})
                f.write(f"**Project:** {project_info.get('name', 'N/A')}\n")
                f.write(f"**Version:** {project_info.get('version', 'N/A')}\n")
                f.write(f"**Generated:** {results.get('timestamp', 'N/A')}\n\n")

                # Summary
                summary = results.get('summary', {})
                f.write(f"## Summary\n\n")
                f.write(f"- **Total Targets:** {summary.get('total_targets', 0)}\n")
                f.write(f"- **Total Evaluations:** {summary.get('total_evaluations', 0)}\n")
                f.write(f"- **Total Test Cases:** {summary.get('total_test_cases', 0)}\n\n")

                # Per-target results
                for target in results.get('targets', []):
                    f.write(f"## Target: {target.get('name', 'Unknown')}\n\n")
                    f.write(f"- **Type:** {target.get('type', 'N/A')}\n")
                    f.write(f"- **Model:** {target.get('model', 'N/A')}\n")
                    f.write(f"- **Provider:** {target.get('provider', 'N/A')}\n")
                    f.write(f"- **Status:** {target.get('status', 'N/A')}\n\n")

                    # Evaluations for this target
                    evaluations = target.get('evaluations', [])
                    if evaluations:
                        f.write(f"### Evaluations ({len(evaluations)})\n\n")

                        for eval_result in evaluations:
                            eval_name = eval_result.get('name', 'Unknown')
                            f.write(f"#### {eval_name}\n\n")
                            f.write(f"- **Dataset:** {eval_result.get('dataset', 'N/A')}\n")
                            f.write(f"- **Dataset Type:** {eval_result.get('dataset_type', 'N/A')}\n")
                            f.write(f"- **Test Cases:** {eval_result.get('num_test_cases', 0)}\n")
                            f.write(f"- **Status:** {eval_result.get('status', 'N/A')}\n\n")

                            # Metrics table
                            if 'metrics_summary' in eval_result:
                                f.write(f"##### Metrics Performance\n\n")
                                f.write(f"| Metric | Avg Score | Success Rate | Status |\n")
                                f.write(f"|--------|-----------|--------------|--------|\n")

                                metrics_summary = eval_result.get('metrics_summary', {})
                                for metric_name, metric_data in metrics_summary.items():
                                    if 'error' in metric_data:
                                        f.write(f"| {metric_name} | N/A | N/A | ‚ùå Failed |\n")
                                    else:
                                        avg_score = metric_data.get('avg_score', 0)
                                        success_rate = metric_data.get('success_rate', 0)

                                        if success_rate >= 0.8:
                                            status = "‚úÖ Excellent"
                                        elif success_rate >= 0.6:
                                            status = "‚ö†Ô∏è Good"
                                        else:
                                            status = "‚ùå Needs Work"

                                        f.write(
                                            f"| {metric_name} | {avg_score:.3f} | {success_rate:.3f} | {status} |\n")

                                f.write(f"\n")

                    # Benchmark results
                    benchmarks = target.get('benchmarks', [])
                    if benchmarks and isinstance(benchmarks, list):
                        # Separate custom and standard benchmarks
                        custom_benchmarks = [b for b in benchmarks if
                                             b.get('backend') == 'custom_eval' or b.get('benchmark_name') == 'custom']
                        standard_benchmarks = [b for b in benchmarks if b.get('backend') != 'custom_eval' and b.get(
                            'benchmark_name') != 'custom']

                        # Custom Evaluation Section
                        if custom_benchmarks:
                            f.write(f"\n### üß™ Custom Evaluation\n\n")

                            for bench_result in custom_benchmarks:
                                bench_name = bench_result.get('benchmark_name', bench_result.get('benchmark', 'Custom'))
                                overall_score = bench_result.get('overall_score', 0.0)
                                status = bench_result.get('status', 'unknown')
                                metadata = bench_result.get('metadata', {})

                                # Overall score with assessment
                                if overall_score >= 0.8:
                                    score_status = "‚úÖ Excellent"
                                elif overall_score >= 0.6:
                                    score_status = "‚ö†Ô∏è Good"
                                elif overall_score >= 0.4:
                                    score_status = "‚ö†Ô∏è Needs Improvement"
                                else:
                                    score_status = "‚ùå Poor"

                                f.write(f"#### {bench_name}\n\n")
                                f.write(f"- **Overall Score:** {overall_score:.2%} ({score_status})\n")
                                f.write(f"- **Status:** {status}\n")
                                if metadata.get('source'):
                                    f.write(f"- **Source:** {metadata.get('source')}\n")
                                if metadata.get('split'):
                                    f.write(f"- **Split:** {metadata.get('split')}\n")
                                f.write(f"\n")

                                # Task Results Summary
                                task_results = bench_result.get('task_results', {})
                                if task_results:
                                    f.write(f"##### Task Results Summary\n\n")
                                    f.write(f"| Task Type | Total | Correct/Success | Accuracy/Avg Score |\n")
                                    f.write(f"|-----------|-------|-----------------|--------------------|\n")

                                    for task_name, task_data in task_results.items():
                                        total = task_data.get('total', 0)

                                        if 'accuracy' in task_data:
                                            # Exact match type
                                            correct = task_data.get('correct', 0)
                                            accuracy = task_data.get('accuracy', 0)
                                            if accuracy >= 0.8:
                                                acc_status = f"‚úÖ {accuracy:.1%}"
                                            elif accuracy >= 0.5:
                                                acc_status = f"‚ö†Ô∏è {accuracy:.1%}"
                                            else:
                                                acc_status = f"‚ùå {accuracy:.1%}"
                                            f.write(f"| {task_name} | {total} | {correct} | {acc_status} |\n")
                                        elif 'avg_score' in task_data:
                                            # Judge type
                                            avg_score = task_data.get('avg_score', 0)
                                            success_rate = task_data.get('success_rate', 0)
                                            if avg_score >= 0.8:
                                                score_status = f"‚úÖ {avg_score:.2f}"
                                            elif avg_score >= 0.5:
                                                score_status = f"‚ö†Ô∏è {avg_score:.2f}"
                                            else:
                                                score_status = f"‚ùå {avg_score:.2f}"
                                            f.write(
                                                f"| {task_name} | {total} | {success_rate:.1%} success | {score_status} |\n")

                                    f.write(f"\n")

                                # Detailed Results
                                detailed_results = bench_result.get('detailed_results', [])
                                if detailed_results:
                                    # Group by eval_type
                                    results_by_type = {}
                                    for detail in detailed_results:
                                        eval_type = detail.get('eval_type', 'unknown')
                                        if eval_type not in results_by_type:
                                            results_by_type[eval_type] = []
                                        results_by_type[eval_type].append(detail)

                                    f.write(f"##### Detailed Results ({len(detailed_results)} test cases)\n\n")

                                    for eval_type, type_results in results_by_type.items():
                                        passed = sum(1 for r in type_results if r.get('success'))
                                        failed = len(type_results) - passed

                                        f.write(
                                            f"###### {eval_type.replace('_', ' ').title()} ({len(type_results)} cases)\n\n")
                                        f.write(f"**Summary:** ‚úÖ {passed} passed, ‚ùå {failed} failed\n\n")

                                        # Group by format if available
                                        results_by_format = {}
                                        for detail in type_results:
                                            fmt = detail.get('format', 'unknown')
                                            if fmt not in results_by_format:
                                                results_by_format[fmt] = []
                                            results_by_format[fmt].append(detail)

                                        for fmt, fmt_results in results_by_format.items():
                                            fmt_passed = sum(1 for r in fmt_results if r.get('success'))
                                            fmt_failed = len(fmt_results) - fmt_passed

                                            if len(results_by_format) > 1:
                                                f.write(f"**Format: {fmt}** (‚úÖ {fmt_passed}, ‚ùå {fmt_failed})\n\n")

                                            # Results table
                                            f.write(
                                                f"| # | Instruction | Expected | Output | Score | Status | Reason |\n")
                                            f.write(
                                                f"|---|-------------|----------|--------|-------|--------|--------|\n")

                                            for detail in fmt_results:
                                                idx = detail.get('original_idx', '?')
                                                instruction = detail.get('instruction', '')
                                                expected = detail.get('expected', '')
                                                output = detail.get('output', '')
                                                raw_output = detail.get('raw_output', '')
                                                score = detail.get('score', 0)
                                                success = detail.get('success', False)
                                                reason = detail.get('reason', '')

                                                # Truncate long strings for table
                                                def truncate(s, max_len=50):
                                                    s = str(s).replace('\n', ' ').replace('|', '\\|')
                                                    return s[:max_len] + '...' if len(s) > max_len else s

                                                status_icon = "‚úÖ" if success else "‚ùå"
                                                score_str = f"{score:.2f}" if isinstance(score, float) else str(score)

                                                f.write(
                                                    f"| {idx} | {truncate(instruction)} | `{truncate(expected, 20)}` | `{truncate(output, 20)}` | {score_str} | {status_icon} | {truncate(reason, 40)} |\n")

                                            f.write(f"\n")

                                        # Failed cases detailed breakdown
                                        failed_cases = [r for r in type_results if not r.get('success')]
                                        if failed_cases:
                                            f.write(
                                                f"<details>\n<summary>üîç Failed Cases Details ({len(failed_cases)})</summary>\n\n")

                                            for detail in failed_cases:
                                                idx = detail.get('original_idx', '?')
                                                instruction = detail.get('instruction', '')
                                                expected = detail.get('expected', '')
                                                output = detail.get('output', '')
                                                raw_output = detail.get('raw_output', '')
                                                score = detail.get('score', 0)
                                                reason = detail.get('reason', '')
                                                fmt = detail.get('format', 'unknown')
                                                criteria = detail.get('criteria', '')

                                                f.write(f"---\n\n")
                                                f.write(f"**Case #{idx}** (Format: {fmt})\n\n")
                                                f.write(f"**Instruction:**\n```\n{instruction}\n```\n\n")
                                                f.write(f"**Expected:** `{expected}`\n\n")
                                                f.write(f"**Output:** `{output}`\n\n")
                                                if raw_output and raw_output != output:
                                                    f.write(f"**Raw Output:**\n```\n{raw_output}\n```\n\n")
                                                f.write(f"**Score:** {score}\n\n")
                                                f.write(f"**Reason:** {reason}\n\n")
                                                if criteria:
                                                    f.write(f"**Criteria:** {criteria}\n\n")

                                            f.write(f"</details>\n\n")

                                    # Overall Analysis
                                    f.write(f"##### Analysis\n\n")

                                    total_cases = len(detailed_results)
                                    total_passed = sum(1 for r in detailed_results if r.get('success'))
                                    total_failed = total_cases - total_passed
                                    pass_rate = total_passed / total_cases if total_cases > 0 else 0

                                    f.write(f"- **Total Test Cases:** {total_cases}\n")
                                    f.write(f"- **Passed:** {total_passed} ({pass_rate:.1%})\n")
                                    f.write(f"- **Failed:** {total_failed} ({1 - pass_rate:.1%})\n\n")

                                    # Breakdown by eval_type
                                    f.write(f"**Performance by Evaluation Type:**\n\n")
                                    for eval_type, type_results in results_by_type.items():
                                        type_passed = sum(1 for r in type_results if r.get('success'))
                                        type_total = len(type_results)
                                        type_rate = type_passed / type_total if type_total > 0 else 0

                                        if type_rate >= 0.8:
                                            type_status = "‚úÖ"
                                        elif type_rate >= 0.5:
                                            type_status = "‚ö†Ô∏è"
                                        else:
                                            type_status = "‚ùå"

                                        f.write(
                                            f"- {eval_type.replace('_', ' ').title()}: {type_status} {type_passed}/{type_total} ({type_rate:.1%})\n")

                                    f.write(f"\n")

                                    # Common failure patterns
                                    if total_failed > 0:
                                        f.write(f"**Common Failure Patterns:**\n\n")

                                        failure_reasons = {}
                                        for detail in detailed_results:
                                            if not detail.get('success'):
                                                reason = detail.get('reason', 'Unknown')
                                                # Normalize reason for grouping
                                                reason_key = reason.split('.')[0] if reason else 'Unknown'
                                                failure_reasons[reason_key] = failure_reasons.get(reason_key, 0) + 1

                                        for reason, count in sorted(failure_reasons.items(), key=lambda x: x[1],
                                                                    reverse=True)[:5]:
                                            f.write(f"- {reason}: {count} case(s)\n")

                                        f.write(f"\n")

                                    # Recommendations
                                    f.write(f"**Recommendations:**\n\n")

                                    if pass_rate >= 0.9:
                                        f.write(
                                            f"‚úÖ Model performance is excellent. Consider expanding test coverage or increasing difficulty.\n\n")
                                    elif pass_rate >= 0.7:
                                        f.write(f"‚ö†Ô∏è Model performance is good but has room for improvement.\n")
                                        if 'exact_match' in results_by_type:
                                            em_results = results_by_type['exact_match']
                                            em_rate = sum(1 for r in em_results if r.get('success')) / len(
                                                em_results) if em_results else 0
                                            if em_rate < 0.8:
                                                f.write(
                                                    f"- Consider improving output formatting to match expected answers more closely.\n")
                                        if 'judge' in results_by_type:
                                            judge_results = results_by_type['judge']
                                            judge_rate = sum(1 for r in judge_results if r.get('success')) / len(
                                                judge_results) if judge_results else 0
                                            if judge_rate < 0.8:
                                                f.write(
                                                    f"- Review judge evaluation criteria and model response quality.\n")
                                        f.write(f"\n")
                                    elif pass_rate >= 0.5:
                                        f.write(f"‚ö†Ô∏è Model performance needs improvement.\n")
                                        f.write(f"- Analyze failed cases to identify systematic issues.\n")
                                        f.write(f"- Consider fine-tuning or prompt engineering.\n\n")
                                    else:
                                        f.write(f"‚ùå Model performance is poor.\n")
                                        f.write(f"- Significant improvements needed before production use.\n")
                                        f.write(f"- Review model capabilities against task requirements.\n")
                                        f.write(f"- Consider using a more capable model.\n\n")

                        # Standard Benchmarks Section
                        if standard_benchmarks:
                            f.write(f"\n### üìä Standard Benchmarks ({len(standard_benchmarks)})\n\n")
                            f.write(f"| Benchmark | Overall Score | Backend | Status |\n")
                            f.write(f"|-----------|---------------|---------|--------|\n")

                            for bench_result in standard_benchmarks:
                                bench_name = bench_result.get('benchmark_name',
                                                              bench_result.get('benchmark', 'Unknown'))
                                overall_score = bench_result.get('overall_score', 0.0)
                                backend = bench_result.get('backend', 'unknown')
                                status = bench_result.get('status', 'unknown')

                                status_emoji = "‚úÖ" if status == "completed" else "‚ùå"
                                f.write(f"| {bench_name} | {overall_score:.4f} | {backend} | {status_emoji} |\n")

                            f.write(f"\n")

                    # Stress testing results
                    if 'stress_testing' in target:
                        f.write(f"\n### ‚ö° Stress Testing\n\n")
                        stress = target['stress_testing']
                        f.write(f"**Status:** {stress.get('status', 'N/A')}\n\n")
                        if 'metrics' in stress:
                            metrics = stress['metrics']
                            f.write(f"| Metric | Value |\n")
                            f.write(f"|--------|-------|\n")
                            f.write(f"| Avg Latency | {metrics.get('avg_latency_ms', 0):.2f} ms |\n")
                            f.write(f"| P95 Latency | {metrics.get('p95_latency_ms', 0):.2f} ms |\n")
                            f.write(f"| P99 Latency | {metrics.get('p99_latency_ms', 0):.2f} ms |\n")
                            f.write(f"| Throughput | {metrics.get('throughput_rps', 0):.2f} RPS |\n")
                            f.write(f"| Error Rate | {metrics.get('error_rate', 0):.2%} |\n")
                            f.write(f"| Total Requests | {metrics.get('total_requests', 0)} |\n")
                        f.write(f"\n")

                    # Red teaming results
                    if 'red_teaming' in target:
                        red_team = target['red_teaming']
                        f.write(f"\n### üî¥ Red Teaming & Security Assessment\n\n")

                        if red_team.get('status') == 'failed':
                            f.write(f"**Status:** ‚ùå Failed\n")
                            f.write(f"**Error:** {red_team.get('error', 'Unknown error')}\n\n")
                        else:
                            f.write(f"**Target:** {red_team.get('target_name', 'N/A')}\n")
                            f.write(f"**Timestamp:** {red_team.get('timestamp', 'N/A')}\n\n")

                            vulnerabilities = red_team.get('vulnerabilities', [])
                            if vulnerabilities:
                                f.write(f"#### Vulnerability Assessment ({len(vulnerabilities)} types tested)\n\n")
                                f.write(
                                    f"| Vulnerability | Total Attacks | Successful | Failed | Success Rate | Severity |\n")
                                f.write(
                                    f"|---------------|---------------|------------|--------|--------------|----------|\n")

                                for vuln in vulnerabilities:
                                    vuln_name = vuln.get('vulnerability_type', 'Unknown')
                                    total = vuln.get('total_attacks', 0)
                                    successful = vuln.get('successful_attacks', 0)
                                    failed = vuln.get('failed_attacks', 0)
                                    success_rate = vuln.get('success_rate', 0)
                                    severity = vuln.get('severity', 'unknown')

                                    if severity == 'critical':
                                        severity_icon = "üî¥ CRITICAL"
                                    elif severity == 'high':
                                        severity_icon = "üü† HIGH"
                                    elif severity == 'medium':
                                        severity_icon = "üü° MEDIUM"
                                    else:
                                        severity_icon = "üü¢ LOW"

                                    if success_rate < 0.2:
                                        rate_status = f"‚úÖ {success_rate:.1%}"
                                    elif success_rate < 0.5:
                                        rate_status = f"‚ö†Ô∏è {success_rate:.1%}"
                                    else:
                                        rate_status = f"‚ùå {success_rate:.1%}"

                                    f.write(
                                        f"| {vuln_name} | {total} | {successful} | {failed} | {rate_status} | {severity_icon} |\n")

                                f.write(f"\n")

                                # Attack breakdown
                                f.write(f"#### Attack Methods Breakdown\n\n")
                                attack_summary = {}
                                for vuln in vulnerabilities:
                                    for attack_type, count in vuln.get('attack_breakdown', {}).items():
                                        attack_summary[attack_type] = attack_summary.get(attack_type, 0) + count

                                if attack_summary:
                                    f.write(f"| Attack Method | Uses |\n")
                                    f.write(f"|---------------|------|\n")
                                    for attack, count in sorted(attack_summary.items(), key=lambda x: x[1],
                                                                reverse=True):
                                        f.write(f"| {attack} | {count} |\n")
                                    f.write(f"\n")

                            # Overall risk summary
                            if vulnerabilities:
                                critical_count = sum(1 for v in vulnerabilities if v.get('severity') == 'critical')
                                high_count = sum(1 for v in vulnerabilities if v.get('severity') == 'high')

                                f.write(f"#### Overall Security Posture\n\n")
                                if critical_count > 0:
                                    f.write(f"‚ö†Ô∏è **Warning:** {critical_count} CRITICAL vulnerabilities detected\n\n")
                                elif high_count > 0:
                                    f.write(f"‚ö†Ô∏è **Caution:** {high_count} HIGH severity vulnerabilities detected\n\n")
                                else:
                                    f.write(f"‚úÖ **Good:** No critical or high severity vulnerabilities detected\n\n")

                    # Guardrails results
                    if 'guardrails' in target:
                        guardrails = target['guardrails']
                        f.write(f"\n### üõ°Ô∏è Guardrails Evaluation\n\n")

                        if guardrails.get('status') == 'failed':
                            f.write(f"**Status:** ‚ùå Failed\n")
                            f.write(f"**Error:** {guardrails.get('error', 'Unknown error')}\n\n")
                        else:
                            f.write(f"**Status:** ‚úÖ Completed\n\n")

                            # Harmful prompts metrics
                            harmful_prompts = guardrails.get('harmful_prompts', {})
                            if harmful_prompts:
                                f.write(f"#### Harmful Prompts Defense\n\n")
                                tested = harmful_prompts.get('tested', 0)
                                refused = harmful_prompts.get('refused', 0)
                                refusal_rate = harmful_prompts.get('refusal_rate', 0)

                                if refusal_rate >= 0.8:
                                    rate_status = f"‚úÖ {refusal_rate:.1%}"
                                    assessment = "Excellent"
                                elif refusal_rate >= 0.6:
                                    rate_status = f"‚ö†Ô∏è {refusal_rate:.1%}"
                                    assessment = "Good"
                                elif refusal_rate >= 0.4:
                                    rate_status = f"‚ö†Ô∏è {refusal_rate:.1%}"
                                    assessment = "Needs Improvement"
                                else:
                                    rate_status = f"‚ùå {refusal_rate:.1%}"
                                    assessment = "Poor"

                                f.write(f"- **Harmful Prompts Tested:** {tested}\n")
                                f.write(f"- **Appropriately Refused:** {refused}\n")
                                f.write(f"- **Refusal Rate:** {rate_status} ({assessment})\n\n")

                            # Safe prompts metrics
                            safe_prompts = guardrails.get('safe_prompts', {})
                            if safe_prompts and safe_prompts.get('tested', 0) > 0:
                                f.write(f"#### Safe Prompts Handling\n\n")
                                tested = safe_prompts.get('tested', 0)
                                allowed = safe_prompts.get('allowed', 0)
                                fp_rate = safe_prompts.get('false_positive_rate', 0)

                                if fp_rate <= 0.1:
                                    fp_status = f"‚úÖ {fp_rate:.1%}"
                                    assessment = "Excellent"
                                elif fp_rate <= 0.2:
                                    fp_status = f"‚ö†Ô∏è {fp_rate:.1%}"
                                    assessment = "Good"
                                elif fp_rate <= 0.4:
                                    fp_status = f"‚ö†Ô∏è {fp_rate:.1%}"
                                    assessment = "Needs Improvement"
                                else:
                                    fp_status = f"‚ùå {fp_rate:.1%}"
                                    assessment = "Poor - Too Restrictive"

                                f.write(f"- **Safe Prompts Tested:** {tested}\n")
                                f.write(f"- **Correctly Allowed:** {allowed}\n")
                                f.write(f"- **False Positive Rate:** {fp_status} ({assessment})\n\n")

                            # Per-vulnerability breakdown
                            refusal_by_vuln = guardrails.get('refusal_by_vulnerability', {})
                            if refusal_by_vuln:
                                f.write(f"#### Refusal Rate by Vulnerability Type\n\n")
                                f.write(f"| Vulnerability | Refusal Rate | Assessment |\n")
                                f.write(f"|---------------|--------------|------------|\n")

                                for vuln, rate in sorted(refusal_by_vuln.items(), key=lambda x: x[1], reverse=True):
                                    if rate >= 0.8:
                                        status = "‚úÖ Strong"
                                    elif rate >= 0.6:
                                        status = "‚ö†Ô∏è Moderate"
                                    else:
                                        status = "‚ùå Weak"

                                    f.write(f"| {vuln} | {rate:.1%} | {status} |\n")

                                f.write(f"\n")

                            # Overall assessment
                            f.write(f"#### Overall Guardrails Assessment\n\n")

                            harmful_rate = guardrails.get('harmful_prompts', {}).get('refusal_rate', 0)
                            fp_rate = guardrails.get('safe_prompts', {}).get('false_positive_rate', 0)

                            if harmful_rate >= 0.8 and fp_rate <= 0.2:
                                f.write(
                                    f"‚úÖ **Excellent:** Model demonstrates strong guardrails with high refusal rate on harmful content and low false positives.\n\n")
                            elif harmful_rate >= 0.6 and fp_rate <= 0.3:
                                f.write(f"‚ö†Ô∏è **Good:** Model has effective guardrails but could be improved.\n\n")
                            elif harmful_rate >= 0.4:
                                f.write(
                                    f"‚ö†Ô∏è **Needs Improvement:** Guardrails are present but insufficient for production use.\n\n")
                            else:
                                f.write(
                                    f"‚ùå **Critical:** Model lacks adequate guardrails and poses significant safety risks.\n\n")

                            # Recommendations
                            f.write(f"**Recommendations:**\n\n")
                            if harmful_rate < 0.8:
                                f.write(
                                    f"- ‚ö†Ô∏è Improve refusal behavior on harmful prompts (currently {harmful_rate:.1%})\n")
                            if fp_rate > 0.2:
                                f.write(f"- ‚ö†Ô∏è Reduce false positives on safe prompts (currently {fp_rate:.1%})\n")
                            if harmful_rate >= 0.8 and fp_rate <= 0.2:
                                f.write(f"- ‚úÖ Current guardrails are well-balanced and effective\n")

                            f.write(f"\n")

                f.write(f"\n---\n\n")
                f.write(f"*Report generated by Surogate Eval*\n")

            logger.success(f"Summary report saved to: {report_file}")

        except Exception as e:
            logger.error(f"Failed to create summary report: {e}")
            import traceback
            logger.debug(traceback.format_exc())

    def _cleanup(self):
        """Cleanup all resources."""
        logger.info("Cleaning up resources")

        for target in self.targets:
            try:
                target.cleanup()
                logger.debug(f"Cleaned up target: {target.name}")
            except Exception as e:
                logger.error(f"Error cleaning up target {target.name}: {e}")

        logger.success("Cleanup complete")

    def get_results(self) -> Dict[str, Any]:
        """Get consolidated evaluation results."""
        return self.consolidated_results

    def get_targets(self) -> List[BaseTarget]:
        """Get configured targets."""
        return self.targets